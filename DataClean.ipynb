{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_sentiment_data(df_sentiments, save_to_csv=False, path=''):\n",
    "    \"\"\"\n",
    "    Analyze sentiment data at minute, hourly and daily granularity, optionally saving the results to CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    df_sentiments (pd.DataFrame): DataFrame containing sentiment data with columns:\n",
    "        ticker, relevance, sentiment, confidence, prob_pos, prob_ntr, prob_neg,\n",
    "        reddit_topic, topicweight, source, sourceweight, author, novelty, comment_count\n",
    "        post_time is assumed to be the index column.\n",
    "    save_to_csv (bool): If True, save the processed dataframes to CSV files.\n",
    "    path: Path to save the data frames if `save_to_csv` is `True`\n",
    "    Returns:\n",
    "    tuple: (ticker_sentiment_minute, market_sentiment_minute, ticker_sentiment_hour, market_sentiment_hour,\n",
    "     ticker_sentiment_daily, market_sentiment_daily) \n",
    "           DataFrames with minute, hourly and daily-level aggregations\n",
    "    \"\"\"\n",
    "    # Reset index if post_time is the index\n",
    "    if 'post_time' not in df_sentiments.columns:\n",
    "        df_sentiments = df_sentiments.reset_index()\n",
    "\n",
    "    # Select relevant columns\n",
    "    df_sentiments = df_sentiments.loc[:, ['post_time', 'ticker',\n",
    "        'relevance', 'sentiment', 'confidence', 'prob_pos', 'prob_ntr',\n",
    "        'prob_neg', 'reddit_topic', 'topicweight', 'source',\n",
    "        'sourceweight', 'author', 'novelty', 'comment_count']]\n",
    "    df_sentiments['post_time'] = pd.to_datetime(df_sentiments['post_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "    # --- Minute-Level Aggregation ---\n",
    "\n",
    "    # Round timestamps to the nearest minute\n",
    "    df_sentiments['minute'] = df_sentiments['post_time'].dt.floor('min')\n",
    "\n",
    "    # Sentiment by ticker and minute\n",
    "    ticker_sentiment_minute = df_sentiments.groupby(['minute', 'ticker']).agg(\n",
    "        obs_count=('prob_pos', 'size'),\n",
    "        mean_pos=('prob_pos', 'mean'),\n",
    "        mean_ntr=('prob_ntr', 'mean'),\n",
    "        mean_neg=('prob_neg', 'mean'),\n",
    "        # Uncomment and adjust if you want weighted calculations\n",
    "        # weighted_pos=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_pos'] * x) / x.sum()),\n",
    "        # weighted_ntr=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_ntr'] * x) / x.sum()),\n",
    "        # weighted_neg=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_neg'] * x) / x.sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Market-wide sentiment by minute\n",
    "    market_sentiment_minute = df_sentiments.groupby('minute').agg(\n",
    "        market_mean_pos=('prob_pos', 'mean'),\n",
    "        market_mean_ntr=('prob_ntr', 'mean'),\n",
    "        market_mean_neg=('prob_neg', 'mean'),\n",
    "        # Uncomment and adjust if you want weighted calculations\n",
    "        # market_weighted_pos=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_pos'] * x) / x.sum()),\n",
    "        # market_weighted_ntr=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_ntr'] * x) / x.sum()),\n",
    "        # market_weighted_neg=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_neg'] * x) / x.sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Drop rows where no sentiment was observed\n",
    "    ticker_sentiment_minute = ticker_sentiment_minute.dropna(subset=['obs_count'])\n",
    "\n",
    "    # Filter out tickers with insufficient observations (adjusted for minute-level data)\n",
    "    minutes_threshold = 2000  # Adjust this value based on your needs\n",
    "    ticker_counts_minute = ticker_sentiment_minute.groupby(\"ticker\")[\"obs_count\"].count()\n",
    "    tickers_over_threshold_minute = ticker_counts_minute[ticker_counts_minute > minutes_threshold].index\n",
    "    ticker_sentiment_minute = ticker_sentiment_minute[ticker_sentiment_minute['ticker'].isin(tickers_over_threshold_minute)]\n",
    "\n",
    "    # --- Hourly-Level Aggregation ---\n",
    "\n",
    "    # Round timestamps to the nearest hour\n",
    "    df_sentiments['time'] = df_sentiments['post_time'].dt.floor('h')\n",
    "\n",
    "    # Sentiment by ticker and hour\n",
    "    ticker_sentiment_hour = df_sentiments.groupby(['time', 'ticker']).agg(\n",
    "        obs_count=('prob_pos', 'size'),\n",
    "        mean_pos=('prob_pos', 'mean'),\n",
    "        mean_ntr=('prob_ntr', 'mean'),\n",
    "        mean_neg=('prob_neg', 'mean'),\n",
    "        # Uncomment and adjust if you want weighted calculations\n",
    "        # weighted_pos=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_pos'] * x) / x.sum()),\n",
    "        # weighted_ntr=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_ntr'] * x) / x.sum()),\n",
    "        # weighted_neg=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_neg'] * x) / x.sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Market-wide sentiment by hour\n",
    "    market_sentiment_hour = df_sentiments.groupby('time').agg(\n",
    "        market_mean_pos=('prob_pos', 'mean'),\n",
    "        market_mean_ntr=('prob_ntr', 'mean'),\n",
    "        market_mean_neg=('prob_neg', 'mean'),\n",
    "        # Uncomment and adjust if you want weighted calculations\n",
    "        # market_weighted_pos=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_pos'] * x) / x.sum()),\n",
    "        # market_weighted_ntr=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_ntr'] * x) / x.sum()),\n",
    "        # market_weighted_neg=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_neg'] * x) / x.sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Drop rows where no sentiment was observed\n",
    "    ticker_sentiment_hour = ticker_sentiment_hour.dropna(subset=['obs_count'])\n",
    "\n",
    "     # Filter out tickers with insufficient observations (adjusted for hourly-level data)\n",
    "    hours_threshold = 500 #Adjust this value based on your needs\n",
    "    ticker_counts_hour = ticker_sentiment_hour.groupby(\"ticker\")[\"obs_count\"].count()\n",
    "    tickers_over_threshold_hour = ticker_counts_hour[ticker_counts_hour > hours_threshold].index\n",
    "    ticker_sentiment_hour = ticker_sentiment_hour[ticker_sentiment_hour['ticker'].isin(tickers_over_threshold_hour)]\n",
    "\n",
    "    # --- Daily-Level Aggregation ---\n",
    "\n",
    "    # Round timestamps to the nearest hour\n",
    "    df_sentiments['date'] = df_sentiments['post_time'].dt.date\n",
    "\n",
    "    # Sentiment by ticker and hour\n",
    "    ticker_sentiment_daily = df_sentiments.groupby(['date', 'ticker']).agg(\n",
    "        obs_count=('prob_pos', 'size'),\n",
    "        mean_pos=('prob_pos', 'mean'),\n",
    "        mean_ntr=('prob_ntr', 'mean'),\n",
    "        mean_neg=('prob_neg', 'mean'),\n",
    "        # Uncomment and adjust if you want weighted calculations\n",
    "        # weighted_pos=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_pos'] * x) / x.sum()),\n",
    "        # weighted_ntr=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_ntr'] * x) / x.sum()),\n",
    "        # weighted_neg=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_neg'] * x) / x.sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Market-wide sentiment by hour\n",
    "    market_sentiment_daily = df_sentiments.groupby('date').agg(\n",
    "        market_mean_pos=('prob_pos', 'mean'),\n",
    "        market_mean_ntr=('prob_ntr', 'mean'),\n",
    "        market_mean_neg=('prob_neg', 'mean'),\n",
    "        # Uncomment and adjust if you want weighted calculations\n",
    "        # market_weighted_pos=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_pos'] * x) / x.sum()),\n",
    "        # market_weighted_ntr=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_ntr'] * x) / x.sum()),\n",
    "        # market_weighted_neg=('confidence', lambda x: np.sum(df_sentiments.loc[x.index, 'prob_neg'] * x) / x.sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Drop rows where no sentiment was observed\n",
    "    ticker_sentiment_daily = ticker_sentiment_daily.dropna(subset=['obs_count'])\n",
    "\n",
    "     # Filter out tickers with insufficient observations (adjusted for daily-level data)\n",
    "    daily_threshold = 10 #Adjust this value based on your needs\n",
    "    ticker_counts_daily = ticker_sentiment_daily.groupby(\"ticker\")[\"obs_count\"].count()\n",
    "    tickers_over_threshold_daily = ticker_counts_daily[ticker_counts_daily > daily_threshold].index\n",
    "    ticker_sentiment_daily = ticker_sentiment_daily[ticker_sentiment_daily['ticker'].isin(tickers_over_threshold_daily)]\n",
    "\n",
    "    if save_to_csv:\n",
    "        ticker_sentiment_minute.to_csv(f'{path}ticker_sentiment_minute.csv', index=False)\n",
    "        market_sentiment_minute.to_csv(f'{path}market_sentiment_minute.csv', index=False)\n",
    "        ticker_sentiment_hour.to_csv(f'{path}ticker_sentiment_hour.csv', index=False)\n",
    "        market_sentiment_hour.to_csv(f'{path}market_sentiment_hour.csv', index=False)\n",
    "        ticker_sentiment_daily.to_csv(f'{path}ticker_sentiment_daily.csv', index=False)\n",
    "        market_sentiment_daily.to_csv(f'{path}market_sentiment_daily.csv', index=False)\n",
    "\n",
    "    return ticker_sentiment_minute, market_sentiment_minute, ticker_sentiment_hour, market_sentiment_hour, ticker_sentiment_daily, market_sentiment_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment data processed and saved to CSVs.\n"
     ]
    }
   ],
   "source": [
    "# Load your sentiment data\n",
    "df_sentiments = pd.read_csv(\"sentiment_raw2.csv\", index_col=0)\n",
    "\n",
    "# Define path for saved csvs\n",
    "path = \"cleaned_data/\"\n",
    "\n",
    "# Create the path if it doesn't exist\n",
    "import os\n",
    "os.makedirs(path, exist_ok = True)\n",
    "\n",
    "\n",
    "# Process and save sentiment data to csv\n",
    "ticker_sentiment_minute, market_sentiment_minute, ticker_sentiment_hour, market_sentiment_hour, ticker_sentiment_daily, market_sentiment_daily = analyze_sentiment_data(df_sentiments, save_to_csv=True, path = path)\n",
    "print(\"Sentiment data processed and saved to CSVs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def fetch_and_save_price_data(symbol, start_date, end_date, interval, path='price_data/'):\n",
    "    \"\"\"\n",
    "    Fetch historical price data from Binance and save it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    symbol (str): The trading pair (e.g., 'BTCUSDT').\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "    interval (str): The time interval ('1m', '1h', '1d').\n",
    "    path (str): Directory to save the price data\n",
    "    \"\"\"\n",
    "    url = 'https://api.binance.com/api/v3/klines'\n",
    "\n",
    "    # Convert dates to timestamps in milliseconds\n",
    "    start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "    end_ts = int(pd.Timestamp(end_date).timestamp() * 1000)\n",
    "\n",
    "    all_data = []\n",
    "    current_ts = start_ts\n",
    "\n",
    "    # Calculate total number of days for progress bar\n",
    "    total_days = (pd.Timestamp(end_date) - pd.Timestamp(start_date)).days\n",
    "    \n",
    "    \n",
    "    with tqdm(total=total_days, desc=f\"Fetching {symbol} {interval} data\") as pbar:\n",
    "      while current_ts < end_ts:\n",
    "        # Parameters for API call\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'startTime': current_ts,\n",
    "            'limit': 1000  # Maximum allowed by Binance\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "          response = requests.get(url, params=params)\n",
    "          response.raise_for_status()  # Raise exception for bad status codes\n",
    "          data = response.json()\n",
    "          \n",
    "          if not data:\n",
    "              break\n",
    "              \n",
    "          all_data.extend(data)\n",
    "          \n",
    "          # Update timestamp for next iteration\n",
    "          if interval == '1m':\n",
    "              current_ts = data[-1][0] + 60000  # Add one minute in milliseconds\n",
    "          elif interval == '1h':\n",
    "              current_ts = data[-1][0] + 60 * 60 * 1000  # Add one hour in milliseconds\n",
    "          elif interval == '1d':\n",
    "              current_ts = data[-1][0] + 60 * 60 * 24 * 1000 #Add one day in milliseconds\n",
    "          \n",
    "          # Update progress bar (approximately)\n",
    "          pbar.update(1)\n",
    "          \n",
    "          # Respect rate limits\n",
    "          time.sleep(0.1)  # 10 requests per second should be safe\n",
    "          \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            time.sleep(5)  # Wait longer on error\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=[\n",
    "        'timestamp', 'open', 'high', 'low', 'close', \n",
    "        'volume', 'close_time', 'quote_volume', 'trades',\n",
    "        'taker_buy_volume', 'taker_buy_quote_volume', 'ignore'\n",
    "    ])\n",
    "\n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "    if interval == '1m':\n",
    "        df['time'] = df['timestamp'].dt.floor('min')\n",
    "    elif interval == '1h':\n",
    "        df['time'] = df['timestamp'].dt.floor('h')\n",
    "    elif interval == '1d':\n",
    "      df['time'] = df['timestamp'].dt.floor('d')\n",
    "    \n",
    "    # Convert price and volume columns to float\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume']:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    # Save to CSV\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_csv(f'{path}{symbol}_{interval}.csv', index=False)\n",
    "    print(f\"Price data for {symbol} at {interval} interval saved to {path}{symbol}_{interval}.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we fetch price data and save to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching BTCUSDT 1m data: 350it [12:24,  2.13s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for BTCUSDT at 1m interval saved to price_data/BTCUSDT_1m.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching BTCUSDT 1h data:   2%|▏         | 6/243 [00:12<08:25,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for BTCUSDT at 1h interval saved to price_data/BTCUSDT_1h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching BTCUSDT 1d data:   0%|          | 1/243 [00:02<09:09,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for BTCUSDT at 1d interval saved to price_data/BTCUSDT_1d.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ETHUSDT 1m data: 350it [12:15,  2.10s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for ETHUSDT at 1m interval saved to price_data/ETHUSDT_1m.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ETHUSDT 1h data:   2%|▏         | 6/243 [00:12<08:25,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for ETHUSDT at 1h interval saved to price_data/ETHUSDT_1h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ETHUSDT 1d data:   0%|          | 1/243 [00:01<07:48,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for ETHUSDT at 1d interval saved to price_data/ETHUSDT_1d.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOGEUSDT 1m data:  57%|█████▋    | 138/243 [28:15<21:30, 12.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m symbols:\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m interval \u001b[38;5;129;01min\u001b[39;00m intervals:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mfetch_and_save_price_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mfetch_and_save_price_data\u001b[0;34m(symbol, start_date, end_date, interval, path)\u001b[0m\n\u001b[1;32m     36\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m: symbol,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m'\u001b[39m: interval,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartTime\u001b[39m\u001b[38;5;124m'\u001b[39m: current_ts,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Maximum allowed by Binance\u001b[39;00m\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m   response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise exception for bad status codes\u001b[39;00m\n\u001b[1;32m     46\u001b[0m   data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:730\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    728\u001b[0m     server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 730\u001b[0m     sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:909\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    907\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 909\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/util/ssl_.py:469\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    465\u001b[0m         context\u001b[38;5;241m.\u001b[39mload_cert_chain(certfile, keyfile, key_password)\n\u001b[1;32m    467\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[0;32m--> 469\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/util/ssl_.py:513\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    510\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define date range and symbols\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-08-31'\n",
    "symbols = ['BTCUSDT', 'ETHUSDT']\n",
    "intervals = ['1m', '1h', '1d'] # Minute, hour, daily intervals\n",
    "\n",
    "# Fetch data and store it\n",
    "for symbol in symbols:\n",
    "  for interval in intervals:\n",
    "    fetch_and_save_price_data(symbol, start_date, end_date, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOGEUSDT 1m data: 350it [13:02,  2.23s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for DOGEUSDT at 1m interval saved to price_data/DOGEUSDT_1m.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOGEUSDT 1h data:   2%|▏         | 6/243 [00:12<08:29,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for DOGEUSDT at 1h interval saved to price_data/DOGEUSDT_1h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOGEUSDT 1d data:   0%|          | 1/243 [00:01<07:13,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data for DOGEUSDT at 1d interval saved to price_data/DOGEUSDT_1d.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define date range and symbols\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-08-31'\n",
    "symbols = ['DOGEUSDT']\n",
    "intervals = ['1m', '1h', '1d'] # Minute, hour, daily intervals\n",
    "\n",
    "# Fetch data and store it\n",
    "for symbol in symbols:\n",
    "  for interval in intervals:\n",
    "    fetch_and_save_price_data(symbol, start_date, end_date, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing BTCUSDT...\n",
      "  Price Interval: 1m\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for BTCUSDT using 1m price data and 1m sentiment data:\n",
      "          sentiment_leads_5m: 0.0028\n",
      "          sentiment_leads_4m: 0.0046\n",
      "          sentiment_leads_3m: -0.0010\n",
      "          sentiment_leads_2m: 0.0024\n",
      "          sentiment_leads_1m: 0.0062\n",
      "          concurrent: 0.0009\n",
      "          price_leads_1m: -0.0014\n",
      "          price_leads_2m: 0.0015\n",
      "          price_leads_3m: 0.0022\n",
      "          price_leads_4m: 0.0007\n",
      "          price_leads_5m: 0.0021\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for BTCUSDT using 1m price data and 1h sentiment data:\n",
      "          sentiment_leads_5m: 0.0015\n",
      "          sentiment_leads_4m: 0.0298\n",
      "          sentiment_leads_3m: 0.0352\n",
      "          sentiment_leads_2m: 0.0315\n",
      "          sentiment_leads_1m: 0.0389\n",
      "          concurrent: 0.0341\n",
      "          price_leads_1m: 0.0465\n",
      "          price_leads_2m: 0.0059\n",
      "          price_leads_3m: -0.0027\n",
      "          price_leads_4m: -0.0169\n",
      "          price_leads_5m: -0.0199\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for BTCUSDT using 1m price data and 1d sentiment data:\n",
      "          sentiment_leads_5m: 0.0462\n",
      "          sentiment_leads_4m: 0.0380\n",
      "          sentiment_leads_3m: 0.0524\n",
      "          sentiment_leads_2m: 0.0894\n",
      "          sentiment_leads_1m: 0.0788\n",
      "          concurrent: 0.2270\n",
      "          price_leads_1m: 0.1749\n",
      "          price_leads_2m: -0.1219\n",
      "          price_leads_3m: -0.0800\n",
      "          price_leads_4m: -0.0330\n",
      "          price_leads_5m: -0.0113\n",
      "  Price Interval: 1h\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for BTCUSDT using 1h price data and 1m sentiment data:\n",
      "          sentiment_leads_2h: -0.0017\n",
      "          sentiment_leads_1h: 0.0088\n",
      "          concurrent: -0.0120\n",
      "          price_leads_1h: 0.0190\n",
      "          price_leads_2h: 0.0130\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for BTCUSDT using 1h price data and 1h sentiment data:\n",
      "          sentiment_leads_2h: 0.0396\n",
      "          sentiment_leads_1h: 0.0333\n",
      "          concurrent: 0.0496\n",
      "          price_leads_1h: 0.0026\n",
      "          price_leads_2h: -0.0003\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for BTCUSDT using 1h price data and 1d sentiment data:\n",
      "          sentiment_leads_2h: 0.0861\n",
      "          sentiment_leads_1h: 0.0668\n",
      "          concurrent: 0.2624\n",
      "          price_leads_1h: 0.1390\n",
      "          price_leads_2h: -0.0913\n",
      "  Price Interval: 1d\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for BTCUSDT using 1d price data and 1m sentiment data:\n",
      "          concurrent: 0.0015\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for BTCUSDT using 1d price data and 1h sentiment data:\n",
      "          concurrent: -0.0023\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for BTCUSDT using 1d price data and 1d sentiment data:\n",
      "          concurrent: 0.1693\n",
      "\n",
      "Analyzing ETHUSDT...\n",
      "  Price Interval: 1m\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for ETHUSDT using 1m price data and 1m sentiment data:\n",
      "          sentiment_leads_5m: 0.0037\n",
      "          sentiment_leads_4m: 0.0000\n",
      "          sentiment_leads_3m: 0.0021\n",
      "          sentiment_leads_2m: 0.0059\n",
      "          sentiment_leads_1m: -0.0011\n",
      "          concurrent: -0.0003\n",
      "          price_leads_1m: -0.0011\n",
      "          price_leads_2m: -0.0027\n",
      "          price_leads_3m: 0.0008\n",
      "          price_leads_4m: -0.0012\n",
      "          price_leads_5m: -0.0049\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for ETHUSDT using 1m price data and 1h sentiment data:\n",
      "          sentiment_leads_5m: 0.0291\n",
      "          sentiment_leads_4m: 0.0386\n",
      "          sentiment_leads_3m: 0.0237\n",
      "          sentiment_leads_2m: 0.0494\n",
      "          sentiment_leads_1m: 0.0337\n",
      "          concurrent: 0.0366\n",
      "          price_leads_1m: 0.0131\n",
      "          price_leads_2m: 0.0015\n",
      "          price_leads_3m: 0.0065\n",
      "          price_leads_4m: -0.0085\n",
      "          price_leads_5m: -0.0158\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for ETHUSDT using 1m price data and 1d sentiment data:\n",
      "          sentiment_leads_5m: 0.0820\n",
      "          sentiment_leads_4m: 0.0662\n",
      "          sentiment_leads_3m: 0.1298\n",
      "          sentiment_leads_2m: 0.1632\n",
      "          sentiment_leads_1m: 0.2191\n",
      "          concurrent: 0.2921\n",
      "          price_leads_1m: 0.1993\n",
      "          price_leads_2m: -0.0188\n",
      "          price_leads_3m: 0.1073\n",
      "          price_leads_4m: -0.0653\n",
      "          price_leads_5m: -0.0733\n",
      "  Price Interval: 1h\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for ETHUSDT using 1h price data and 1m sentiment data:\n",
      "          sentiment_leads_2h: 0.0295\n",
      "          sentiment_leads_1h: 0.0054\n",
      "          concurrent: 0.0126\n",
      "          price_leads_1h: -0.0629\n",
      "          price_leads_2h: -0.0542\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for ETHUSDT using 1h price data and 1h sentiment data:\n",
      "          sentiment_leads_2h: 0.0356\n",
      "          sentiment_leads_1h: 0.0338\n",
      "          concurrent: 0.0160\n",
      "          price_leads_1h: 0.0017\n",
      "          price_leads_2h: 0.0087\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for ETHUSDT using 1h price data and 1d sentiment data:\n",
      "          sentiment_leads_2h: 0.1665\n",
      "          sentiment_leads_1h: 0.2271\n",
      "          concurrent: 0.3056\n",
      "          price_leads_1h: 0.1613\n",
      "          price_leads_2h: 0.0096\n",
      "  Price Interval: 1d\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for ETHUSDT using 1d price data and 1m sentiment data:\n",
      "          concurrent: 0.2323\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for ETHUSDT using 1d price data and 1h sentiment data:\n",
      "          concurrent: -0.0284\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for ETHUSDT using 1d price data and 1d sentiment data:\n",
      "          concurrent: 0.2001\n",
      "\n",
      "Analyzing DOGEUSDT...\n",
      "  Price Interval: 1m\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for DOGEUSDT using 1m price data and 1m sentiment data:\n",
      "          sentiment_leads_5m: 0.0151\n",
      "          sentiment_leads_4m: 0.0188\n",
      "          sentiment_leads_3m: 0.0051\n",
      "          sentiment_leads_2m: -0.0123\n",
      "          sentiment_leads_1m: -0.0083\n",
      "          concurrent: -0.0118\n",
      "          price_leads_1m: -0.0356\n",
      "          price_leads_2m: -0.0011\n",
      "          price_leads_3m: 0.0477\n",
      "          price_leads_4m: -0.0105\n",
      "          price_leads_5m: 0.0171\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for DOGEUSDT using 1m price data and 1h sentiment data:\n",
      "          sentiment_leads_5m: 0.0122\n",
      "          sentiment_leads_4m: 0.0211\n",
      "          sentiment_leads_3m: 0.0183\n",
      "          sentiment_leads_2m: -0.0096\n",
      "          sentiment_leads_1m: -0.0014\n",
      "          concurrent: -0.0106\n",
      "          price_leads_1m: -0.0415\n",
      "          price_leads_2m: 0.0067\n",
      "          price_leads_3m: 0.0219\n",
      "          price_leads_4m: 0.0051\n",
      "          price_leads_5m: 0.0037\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for DOGEUSDT using 1m price data and 1d sentiment data:\n",
      "          sentiment_leads_5m: -0.0010\n",
      "          sentiment_leads_4m: 0.0625\n",
      "          sentiment_leads_3m: 0.0042\n",
      "          sentiment_leads_2m: 0.0714\n",
      "          sentiment_leads_1m: 0.0320\n",
      "          concurrent: 0.0702\n",
      "          price_leads_1m: -0.0644\n",
      "          price_leads_2m: 0.1202\n",
      "          price_leads_3m: -0.0356\n",
      "          price_leads_4m: -0.0009\n",
      "          price_leads_5m: -0.1044\n",
      "  Price Interval: 1h\n",
      "    Sentiment Interval: 1m\n",
      "       Correlations for DOGEUSDT using 1h price data and 1m sentiment data:\n",
      "          sentiment_leads_2h: -0.0140\n",
      "          sentiment_leads_1h: 0.0978\n",
      "          concurrent: 0.0660\n",
      "          price_leads_1h: -0.1230\n",
      "          price_leads_2h: -0.1547\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for DOGEUSDT using 1h price data and 1h sentiment data:\n",
      "          sentiment_leads_2h: 0.0142\n",
      "          sentiment_leads_1h: -0.0100\n",
      "          concurrent: -0.0493\n",
      "          price_leads_1h: -0.0061\n",
      "          price_leads_2h: 0.0053\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for DOGEUSDT using 1h price data and 1d sentiment data:\n",
      "          sentiment_leads_2h: 0.0634\n",
      "          sentiment_leads_1h: 0.0340\n",
      "          concurrent: 0.0800\n",
      "          price_leads_1h: -0.0469\n",
      "          price_leads_2h: 0.0884\n",
      "  Price Interval: 1d\n",
      "    Sentiment Interval: 1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2991: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Correlations for DOGEUSDT using 1d price data and 1m sentiment data:\n",
      "          concurrent: nan\n",
      "    Sentiment Interval: 1h\n",
      "       Correlations for DOGEUSDT using 1d price data and 1h sentiment data:\n",
      "          concurrent: -0.0729\n",
      "    Sentiment Interval: 1d\n",
      "       Correlations for DOGEUSDT using 1d price data and 1d sentiment data:\n",
      "          concurrent: -0.0616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def analyze_price_sentiment_relationship(price_df, sentiment_df, symbol, interval):\n",
    "    \"\"\"\n",
    "    Analyze the relationship between price movements and sentiment\n",
    "    \n",
    "    Parameters:\n",
    "    - price_df: DataFrame with price data\n",
    "    - sentiment_df: DataFrame with sentiment data\n",
    "    - symbol: crypto symbol (e.g., 'BTC')\n",
    "    - interval: '1m', '1h', '1d'\n",
    "\n",
    "    Returns:\n",
    "    - correlations: A dict where keys are lags and values are the correlation coefficients\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge price and sentiment data\n",
    "    merged_df = pd.merge(\n",
    "      price_df,\n",
    "      sentiment_df,\n",
    "      on='time',\n",
    "      how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate price returns\n",
    "    merged_df['ret'] = merged_df['close'].pct_change()\n",
    "    \n",
    "    # Calculate correlations at different lags\n",
    "    correlations = {}\n",
    "    if interval == '1m':\n",
    "      for lag in range(-5, 6):  # -5 to +5 minutes\n",
    "            if lag < 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(abs(lag)))\n",
    "                correlations[f'sentiment_leads_{abs(lag)}m'] = corr\n",
    "            elif lag > 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(-lag))\n",
    "                correlations[f'price_leads_{lag}m'] = corr\n",
    "            else:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "                correlations['concurrent'] = corr\n",
    "    elif interval == '1h':\n",
    "      for lag in range(-2, 3):  # -2 to +2 hours\n",
    "            if lag < 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(abs(lag)))\n",
    "                correlations[f'sentiment_leads_{abs(lag)}h'] = corr\n",
    "            elif lag > 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(-lag))\n",
    "                correlations[f'price_leads_{lag}h'] = corr\n",
    "            else:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "                correlations['concurrent'] = corr\n",
    "    elif interval == '1d':\n",
    "        corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "        correlations['concurrent'] = corr\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "\n",
    "# Define path for the price and sentiment data\n",
    "price_data_path = \"price_data/\"\n",
    "sentiment_data_path = \"cleaned_data/\"\n",
    "\n",
    "# Define tokens and intervals\n",
    "symbols = ['BTCUSDT', 'ETHUSDT', 'DOGEUSDT']\n",
    "intervals = ['1m', '1h', '1d'] # Minute, hour, daily intervals\n",
    "\n",
    "# Analysis function per token\n",
    "for symbol in symbols:\n",
    "    print(f\"\\nAnalyzing {symbol}...\")\n",
    "    \n",
    "    for price_interval in intervals:\n",
    "        print(f\"  Price Interval: {price_interval}\")\n",
    "        \n",
    "        # Load price data\n",
    "        price_df = pd.read_csv(f'{price_data_path}{symbol}_{price_interval}.csv')\n",
    "        price_df['time'] = pd.to_datetime(price_df['time'])\n",
    "\n",
    "        \n",
    "        for sentiment_interval in intervals:\n",
    "            print(f\"    Sentiment Interval: {sentiment_interval}\")\n",
    "            \n",
    "            # Load sentiment data\n",
    "            if sentiment_interval == '1m':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['minute'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1h':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_hour.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['time'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1d':\n",
    "                sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_daily.csv')\n",
    "                sentiment_df['time'] = pd.to_datetime(sentiment_df['date']).dt.date\n",
    "                sentiment_df['time'] = pd.to_datetime(sentiment_df['time'])\n",
    "                sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "\n",
    "            # Analyze relationships\n",
    "            correlations = analyze_price_sentiment_relationship(price_df, sentiment_df, symbol.replace('USDT', ''), price_interval)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"       Correlations for {symbol} using {price_interval} price data and {sentiment_interval} sentiment data:\")\n",
    "            for k, v in correlations.items():\n",
    "                print(f\"          {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap for BTCUSDT saved to visuals/BTCUSDT_correlation_heatmap.png\n",
      "Heatmap for ETHUSDT saved to visuals/ETHUSDT_correlation_heatmap.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2991: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap for DOGEUSDT saved to visuals/DOGEUSDT_correlation_heatmap.png\n",
      "Top correlation table for BTCUSDT saved to visuals/top_correlations_BTCUSDT.png\n",
      "Top correlation table for ETHUSDT saved to visuals/top_correlations_ETHUSDT.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2991: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlation table for DOGEUSDT saved to visuals/top_correlations_DOGEUSDT.png\n",
      "Intraday sentiment chart saved to visuals/intraday_sentiment_patterns.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def analyze_price_sentiment_relationship(price_df, sentiment_df, symbol, interval):\n",
    "    \"\"\"\n",
    "    Analyze the relationship between price movements and sentiment\n",
    "    \n",
    "    Parameters:\n",
    "    - price_df: DataFrame with price data\n",
    "    - sentiment_df: DataFrame with sentiment data\n",
    "    - symbol: crypto symbol (e.g., 'BTC')\n",
    "    - interval: '1m', '1h', '1d'\n",
    "\n",
    "    Returns:\n",
    "    - correlations: A dict where keys are lags and values are the correlation coefficients\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge price and sentiment data\n",
    "    merged_df = pd.merge(\n",
    "      price_df,\n",
    "      sentiment_df,\n",
    "      on='time',\n",
    "      how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate price returns\n",
    "    merged_df['ret'] = merged_df['close'].pct_change()\n",
    "    \n",
    "    # Calculate correlations at different lags\n",
    "    correlations = {}\n",
    "    if interval == '1m':\n",
    "      for lag in range(-5, 6):  # -5 to +5 minutes\n",
    "            if lag < 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(abs(lag)))\n",
    "                correlations[f'sentiment_leads_{abs(lag)}m'] = corr\n",
    "            elif lag > 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(-lag))\n",
    "                correlations[f'price_leads_{lag}m'] = corr\n",
    "            else:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "                correlations['concurrent'] = corr\n",
    "    elif interval == '1h':\n",
    "      for lag in range(-2, 3):  # -2 to +2 hours\n",
    "            if lag < 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(abs(lag)))\n",
    "                correlations[f'sentiment_leads_{abs(lag)}h'] = corr\n",
    "            elif lag > 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(-lag))\n",
    "                correlations[f'price_leads_{lag}h'] = corr\n",
    "            else:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "                correlations['concurrent'] = corr\n",
    "    elif interval == '1d':\n",
    "        corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "        correlations['concurrent'] = corr\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def create_correlation_heatmap(symbol, price_intervals, sentiment_intervals, price_data_path, sentiment_data_path, save_path='visuals/'):\n",
    "    \"\"\"\n",
    "    Generates and saves correlation heatmaps for different time intervals.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): Cryptocurrency symbol ('BTCUSDT', 'ETHUSDT', 'DOGEUSDT').\n",
    "        price_intervals (list): List of price intervals ('1m', '1h', '1d').\n",
    "        sentiment_intervals (list): List of sentiment intervals ('1m', '1h', '1d').\n",
    "        price_data_path (str): Path to the directory with price data CSVs.\n",
    "        sentiment_data_path (str): Path to the directory with sentiment data CSVs.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_correlations = {}\n",
    "\n",
    "    for price_interval in price_intervals:\n",
    "        all_correlations[price_interval] = {}\n",
    "        # Load price data\n",
    "        price_df = pd.read_csv(f'{price_data_path}{symbol}_{price_interval}.csv')\n",
    "        price_df['time'] = pd.to_datetime(price_df['time'])\n",
    "\n",
    "\n",
    "        for sentiment_interval in sentiment_intervals:\n",
    "            # Load sentiment data\n",
    "            if sentiment_interval == '1m':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['minute'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1h':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_hour.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['time'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1d':\n",
    "                sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_daily.csv')\n",
    "                sentiment_df['time'] = pd.to_datetime(sentiment_df['date'])\n",
    "                sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "                \n",
    "\n",
    "            # Analyze relationships\n",
    "            correlations = analyze_price_sentiment_relationship(price_df, sentiment_df, symbol.replace('USDT', ''), price_interval)\n",
    "            \n",
    "            all_correlations[price_interval][sentiment_interval] = correlations\n",
    "\n",
    "    # Convert to DataFrame for Heatmap\n",
    "    data = []\n",
    "    for price_interval, sent_intervals in all_correlations.items():\n",
    "      for sent_interval, corrs in sent_intervals.items():\n",
    "        for key, value in corrs.items():\n",
    "          data.append({'Price Interval':price_interval,\n",
    "                       'Sentiment Interval':sent_interval,\n",
    "                       'Lag':key,\n",
    "                       'Correlation':value})\n",
    "\n",
    "    df_heatmap = pd.DataFrame(data)\n",
    "\n",
    "    # Pivot the dataframe to create the heatmap\n",
    "    df_heatmap = df_heatmap.pivot_table(index = ['Price Interval', 'Lag'], columns='Sentiment Interval', values='Correlation')\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df_heatmap, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Heatmap for {symbol}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save heatmap\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(f'{save_path}{symbol}_correlation_heatmap.png')\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "    print(f\"Heatmap for {symbol} saved to {save_path}{symbol}_correlation_heatmap.png\")\n",
    "\n",
    "def create_top_correlations_table(symbol, price_data_path, sentiment_data_path, save_path='visuals/'):\n",
    "    \"\"\"\n",
    "    Generates a table of the top 5 positive and negative correlations between price and sentiment\n",
    "    at the daily level, and saves it to a png file.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): Cryptocurrency symbol ('BTCUSDT', 'ETHUSDT', 'DOGEUSDT').\n",
    "        price_data_path (str): Path to the directory with price data CSVs.\n",
    "        sentiment_data_path (str): Path to the directory with sentiment data CSVs.\n",
    "        save_path (str): Path to save the output.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    price_interval = '1d'\n",
    "    price_df = pd.read_csv(f'{price_data_path}{symbol}_{price_interval}.csv')\n",
    "    price_df['time'] = pd.to_datetime(price_df['time'])\n",
    "    all_correlations = {}\n",
    "    for sentiment_interval in ['1m', '1h', '1d']:\n",
    "            # Load sentiment data\n",
    "            if sentiment_interval == '1m':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['minute'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1h':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_hour.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['time'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1d':\n",
    "                sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_daily.csv')\n",
    "                sentiment_df['time'] = pd.to_datetime(sentiment_df['date'])\n",
    "                sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "                \n",
    "            # Analyze relationships\n",
    "            correlations = analyze_price_sentiment_relationship(price_df, sentiment_df, symbol.replace('USDT', ''), price_interval)\n",
    "            \n",
    "            all_correlations[sentiment_interval] = correlations\n",
    "\n",
    "    # Convert to DataFrame for Heatmap\n",
    "    data = []\n",
    "    for sent_interval, corrs in all_correlations.items():\n",
    "      for key, value in corrs.items():\n",
    "        data.append({'Sentiment Interval':sent_interval,\n",
    "                     'Lag':key,\n",
    "                     'Correlation':value})\n",
    "\n",
    "    df_correlations = pd.DataFrame(data)\n",
    "    df_correlations.sort_values('Correlation', ascending = False, inplace = True)\n",
    "\n",
    "    # Select only top and bottom 5 correlations\n",
    "    df_top_corr = df_correlations.head(5)\n",
    "    df_bottom_corr = df_correlations.tail(5)\n",
    "    df_final = pd.concat([df_top_corr, df_bottom_corr])\n",
    "\n",
    "    # Create table\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.axis('off')\n",
    "    plt.axis('tight')\n",
    "    table = plt.table(cellText = df_final.values, colLabels=df_final.columns, loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2,1.2)\n",
    "    plt.title(f'Top Correlations for {symbol}')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(f\"{save_path}top_correlations_{symbol}.png\")\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "    print(f\"Top correlation table for {symbol} saved to {save_path}top_correlations_{symbol}.png\")\n",
    "\n",
    "def create_intraday_sentiment_chart(sentiment_data_path, save_path = 'visuals/'):\n",
    "  \"\"\"\n",
    "    Generates and saves a chart of intraday sentiment patterns.\n",
    "\n",
    "    Parameters:\n",
    "        sentiment_data_path (str): Path to the directory with sentiment data CSVs.\n",
    "    \"\"\"\n",
    "  # Load sentiment data and calculate intraday patterns\n",
    "  sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "  sentiment_df['minute'] = pd.to_datetime(sentiment_df['minute'])\n",
    "  sentiment_df['hour'] = sentiment_df['minute'].dt.hour\n",
    "\n",
    "  hourly_stats = sentiment_df.groupby('hour').agg({\n",
    "      'mean_pos': ['mean', 'std'],\n",
    "      'obs_count': 'mean'\n",
    "  }).round(3)\n",
    "\n",
    "  # Create the chart\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  plt.plot(hourly_stats.index, hourly_stats['mean_pos']['mean'], label = 'Mean Pos Sentiment')\n",
    "  plt.fill_between(hourly_stats.index,\n",
    "                   hourly_stats['mean_pos']['mean'] + hourly_stats['mean_pos']['std'],\n",
    "                   hourly_stats['mean_pos']['mean'] - hourly_stats['mean_pos']['std'],\n",
    "                   alpha = 0.3)\n",
    "  plt.xlabel(\"Hour of Day\")\n",
    "  plt.ylabel(\"Mean Positive Sentiment\")\n",
    "  plt.title(\"Intraday Sentiment Patterns\")\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.xticks(range(0,24))\n",
    "  plt.tight_layout()\n",
    "    # Save heatmap\n",
    "  os.makedirs(save_path, exist_ok=True)\n",
    "  plt.savefig(f'{save_path}intraday_sentiment_patterns.png')\n",
    "  plt.close()\n",
    "  print(f\"Intraday sentiment chart saved to {save_path}intraday_sentiment_patterns.png\")\n",
    "    \n",
    "# Define paths\n",
    "price_data_path = \"price_data/\"\n",
    "sentiment_data_path = \"cleaned_data/\"\n",
    "save_path = \"visuals/\"\n",
    "# Create the save path if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok = True)\n",
    "\n",
    "# Define symbols and intervals\n",
    "symbols = ['BTCUSDT', 'ETHUSDT', 'DOGEUSDT']\n",
    "price_intervals = ['1m', '1h', '1d']\n",
    "sentiment_intervals = ['1m', '1h', '1d']\n",
    "\n",
    "# Generate heatmaps\n",
    "for symbol in symbols:\n",
    "  create_correlation_heatmap(symbol, price_intervals, sentiment_intervals, price_data_path, sentiment_data_path)\n",
    "# Generate top correlation tables\n",
    "for symbol in symbols:\n",
    "    create_top_correlations_table(symbol, price_data_path, sentiment_data_path)\n",
    "# Generate the intraday sentiment chart\n",
    "create_intraday_sentiment_chart(sentiment_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap for BTCUSDT saved to visuals/BTCUSDT_correlation_heatmap.png\n",
      "Heatmap data for BTCUSDT saved to visuals/BTCUSDT_heatmap_data.csv\n",
      "Heatmap for ETHUSDT saved to visuals/ETHUSDT_correlation_heatmap.png\n",
      "Heatmap data for ETHUSDT saved to visuals/ETHUSDT_heatmap_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2991: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap for DOGEUSDT saved to visuals/DOGEUSDT_correlation_heatmap.png\n",
      "Heatmap data for DOGEUSDT saved to visuals/DOGEUSDT_heatmap_data.csv\n",
      "Top correlation table for BTCUSDT saved to visuals/top_correlations_BTCUSDT.png\n",
      "Top correlation data for BTCUSDT saved to visuals/top_correlations_BTCUSDT_data.csv\n",
      "Top correlation table for ETHUSDT saved to visuals/top_correlations_ETHUSDT.png\n",
      "Top correlation data for ETHUSDT saved to visuals/top_correlations_ETHUSDT_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2991: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2848: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlation table for DOGEUSDT saved to visuals/top_correlations_DOGEUSDT.png\n",
      "Top correlation data for DOGEUSDT saved to visuals/top_correlations_DOGEUSDT_data.csv\n",
      "Intraday sentiment chart saved to visuals/intraday_sentiment_patterns.png\n",
      "Intraday sentiment chart data saved to visuals/intraday_sentiment_patterns_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def analyze_price_sentiment_relationship(price_df, sentiment_df, symbol, interval):\n",
    "    \"\"\"\n",
    "    Analyze the relationship between price movements and sentiment\n",
    "    \n",
    "    Parameters:\n",
    "    - price_df: DataFrame with price data\n",
    "    - sentiment_df: DataFrame with sentiment data\n",
    "    - symbol: crypto symbol (e.g., 'BTC')\n",
    "    - interval: '1m', '1h', '1d'\n",
    "\n",
    "    Returns:\n",
    "    - correlations: A dict where keys are lags and values are the correlation coefficients\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge price and sentiment data\n",
    "    merged_df = pd.merge(\n",
    "      price_df,\n",
    "      sentiment_df,\n",
    "      on='time',\n",
    "      how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate price returns\n",
    "    merged_df['ret'] = merged_df['close'].pct_change()\n",
    "    \n",
    "    # Calculate correlations at different lags\n",
    "    correlations = {}\n",
    "    if interval == '1m':\n",
    "      for lag in range(-5, 6):  # -5 to +5 minutes\n",
    "            if lag < 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(abs(lag)))\n",
    "                correlations[f'sentiment_leads_{abs(lag)}m'] = corr\n",
    "            elif lag > 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(-lag))\n",
    "                correlations[f'price_leads_{lag}m'] = corr\n",
    "            else:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "                correlations['concurrent'] = corr\n",
    "    elif interval == '1h':\n",
    "      for lag in range(-2, 3):  # -2 to +2 hours\n",
    "            if lag < 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(abs(lag)))\n",
    "                correlations[f'sentiment_leads_{abs(lag)}h'] = corr\n",
    "            elif lag > 0:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'].shift(-lag))\n",
    "                correlations[f'price_leads_{lag}h'] = corr\n",
    "            else:\n",
    "                corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "                correlations['concurrent'] = corr\n",
    "    elif interval == '1d':\n",
    "        corr = merged_df['mean_pos'].corr(merged_df['ret'])\n",
    "        correlations['concurrent'] = corr\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def create_correlation_heatmap(symbol, price_intervals, sentiment_intervals, price_data_path, sentiment_data_path, save_path='visuals/'):\n",
    "    \"\"\"\n",
    "    Generates and saves correlation heatmaps for different time intervals.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): Cryptocurrency symbol ('BTCUSDT', 'ETHUSDT', 'DOGEUSDT').\n",
    "        price_intervals (list): List of price intervals ('1m', '1h', '1d').\n",
    "        sentiment_intervals (list): List of sentiment intervals ('1m', '1h', '1d').\n",
    "        price_data_path (str): Path to the directory with price data CSVs.\n",
    "        sentiment_data_path (str): Path to the directory with sentiment data CSVs.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_correlations = {}\n",
    "    all_data = []\n",
    "    for price_interval in price_intervals:\n",
    "        all_correlations[price_interval] = {}\n",
    "        # Load price data\n",
    "        price_df = pd.read_csv(f'{price_data_path}{symbol}_{price_interval}.csv')\n",
    "        price_df['time'] = pd.to_datetime(price_df['time'])\n",
    "\n",
    "\n",
    "        for sentiment_interval in sentiment_intervals:\n",
    "            # Load sentiment data\n",
    "            if sentiment_interval == '1m':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['minute'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1h':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_hour.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['time'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1d':\n",
    "                sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_daily.csv')\n",
    "                sentiment_df['time'] = pd.to_datetime(sentiment_df['date'])\n",
    "                sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "                \n",
    "            # Analyze relationships\n",
    "            correlations = analyze_price_sentiment_relationship(price_df, sentiment_df, symbol.replace('USDT', ''), price_interval)\n",
    "            \n",
    "            all_correlations[price_interval][sentiment_interval] = correlations\n",
    "\n",
    "    # Convert to DataFrame for Heatmap\n",
    "    \n",
    "    for price_interval, sent_intervals in all_correlations.items():\n",
    "      for sent_interval, corrs in sent_intervals.items():\n",
    "        for key, value in corrs.items():\n",
    "          all_data.append({'Price Interval':price_interval,\n",
    "                       'Sentiment Interval':sent_interval,\n",
    "                       'Lag':key,\n",
    "                       'Correlation':value})\n",
    "\n",
    "    df_heatmap = pd.DataFrame(all_data)\n",
    "\n",
    "    # Pivot the dataframe to create the heatmap\n",
    "    df_heatmap_pivot = df_heatmap.pivot_table(index = ['Price Interval', 'Lag'], columns='Sentiment Interval', values='Correlation')\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df_heatmap_pivot, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Heatmap for {symbol}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save heatmap\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(f'{save_path}{symbol}_correlation_heatmap.png')\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "    df_heatmap.to_csv(f'{save_path}{symbol}_heatmap_data.csv', index=False)\n",
    "\n",
    "    print(f\"Heatmap for {symbol} saved to {save_path}{symbol}_correlation_heatmap.png\")\n",
    "    print(f\"Heatmap data for {symbol} saved to {save_path}{symbol}_heatmap_data.csv\")\n",
    "\n",
    "def create_top_correlations_table(symbol, price_data_path, sentiment_data_path, save_path='visuals/'):\n",
    "    \"\"\"\n",
    "    Generates a table of the top 5 positive and negative correlations between price and sentiment\n",
    "    at the daily level, and saves it to a png file.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): Cryptocurrency symbol ('BTCUSDT', 'ETHUSDT', 'DOGEUSDT').\n",
    "        price_data_path (str): Path to the directory with price data CSVs.\n",
    "        sentiment_data_path (str): Path to the directory with sentiment data CSVs.\n",
    "        save_path (str): Path to save the output.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    price_interval = '1d'\n",
    "    price_df = pd.read_csv(f'{price_data_path}{symbol}_{price_interval}.csv')\n",
    "    price_df['time'] = pd.to_datetime(price_df['time'])\n",
    "    all_correlations = {}\n",
    "    for sentiment_interval in ['1m', '1h', '1d']:\n",
    "            # Load sentiment data\n",
    "            if sentiment_interval == '1m':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['minute'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1h':\n",
    "              sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_hour.csv')\n",
    "              sentiment_df['time'] = pd.to_datetime(sentiment_df['time'])\n",
    "              sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "            elif sentiment_interval == '1d':\n",
    "                sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_daily.csv')\n",
    "                sentiment_df['time'] = pd.to_datetime(sentiment_df['date'])\n",
    "                sentiment_df = sentiment_df[sentiment_df['ticker'] == symbol.replace('USDT', '')]\n",
    "                \n",
    "            # Analyze relationships\n",
    "            correlations = analyze_price_sentiment_relationship(price_df, sentiment_df, symbol.replace('USDT', ''), price_interval)\n",
    "            \n",
    "            all_correlations[sentiment_interval] = correlations\n",
    "\n",
    "    # Convert to DataFrame for Heatmap\n",
    "    data = []\n",
    "    for sent_interval, corrs in all_correlations.items():\n",
    "      for key, value in corrs.items():\n",
    "        data.append({'Sentiment Interval':sent_interval,\n",
    "                     'Lag':key,\n",
    "                     'Correlation':value})\n",
    "\n",
    "    df_correlations = pd.DataFrame(data)\n",
    "    df_correlations.sort_values('Correlation', ascending = False, inplace = True)\n",
    "\n",
    "    # Select only top and bottom 5 correlations\n",
    "    df_top_corr = df_correlations.head(5)\n",
    "    df_bottom_corr = df_correlations.tail(5)\n",
    "    df_final = pd.concat([df_top_corr, df_bottom_corr])\n",
    "    df_final.to_csv(f\"{save_path}top_correlations_{symbol}_data.csv\", index=False)\n",
    "\n",
    "    # Create table\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.axis('off')\n",
    "    plt.axis('tight')\n",
    "    table = plt.table(cellText = df_final.values, colLabels=df_final.columns, loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2,1.2)\n",
    "    plt.title(f'Top Correlations for {symbol}')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(f\"{save_path}top_correlations_{symbol}.png\")\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "    print(f\"Top correlation table for {symbol} saved to {save_path}top_correlations_{symbol}.png\")\n",
    "    print(f\"Top correlation data for {symbol} saved to {save_path}top_correlations_{symbol}_data.csv\")\n",
    "\n",
    "def create_intraday_sentiment_chart(sentiment_data_path, save_path = 'visuals/'):\n",
    "  \"\"\"\n",
    "    Generates and saves a chart of intraday sentiment patterns.\n",
    "\n",
    "    Parameters:\n",
    "        sentiment_data_path (str): Path to the directory with sentiment data CSVs.\n",
    "    \"\"\"\n",
    "  # Load sentiment data and calculate intraday patterns\n",
    "  sentiment_df = pd.read_csv(f'{sentiment_data_path}ticker_sentiment_minute.csv')\n",
    "  sentiment_df['minute'] = pd.to_datetime(sentiment_df['minute'])\n",
    "  sentiment_df['hour'] = sentiment_df['minute'].dt.hour\n",
    "\n",
    "  hourly_stats = sentiment_df.groupby('hour').agg({\n",
    "      'mean_pos': ['mean', 'std'],\n",
    "      'obs_count': 'mean'\n",
    "  }).round(3)\n",
    "  hourly_stats.reset_index(inplace=True)\n",
    "\n",
    "  # Create the chart\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  plt.plot(hourly_stats['hour'], hourly_stats['mean_pos']['mean'], label = 'Mean Pos Sentiment')\n",
    "  plt.fill_between(hourly_stats['hour'],\n",
    "                   hourly_stats['mean_pos']['mean'] + hourly_stats['mean_pos']['std'],\n",
    "                   hourly_stats['mean_pos']['mean'] - hourly_stats['mean_pos']['std'],\n",
    "                   alpha = 0.3)\n",
    "  plt.xlabel(\"Hour of Day\")\n",
    "  plt.ylabel(\"Mean Positive Sentiment\")\n",
    "  plt.title(\"Intraday Sentiment Patterns\")\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.xticks(range(0,24))\n",
    "  plt.tight_layout()\n",
    "    # Save heatmap\n",
    "  os.makedirs(save_path, exist_ok=True)\n",
    "  plt.savefig(f'{save_path}intraday_sentiment_patterns.png')\n",
    "  plt.close()\n",
    "\n",
    "  hourly_stats.to_csv(f'{save_path}intraday_sentiment_patterns_data.csv', index=False)\n",
    "\n",
    "  print(f\"Intraday sentiment chart saved to {save_path}intraday_sentiment_patterns.png\")\n",
    "  print(f\"Intraday sentiment chart data saved to {save_path}intraday_sentiment_patterns_data.csv\")\n",
    "    \n",
    "# Define paths\n",
    "price_data_path = \"price_data/\"\n",
    "sentiment_data_path = \"cleaned_data/\"\n",
    "save_path = \"visuals/\"\n",
    "# Create the save path if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok = True)\n",
    "\n",
    "# Define symbols and intervals\n",
    "symbols = ['BTCUSDT', 'ETHUSDT', 'DOGEUSDT']\n",
    "price_intervals = ['1m', '1h', '1d']\n",
    "sentiment_intervals = ['1m', '1h', '1d']\n",
    "\n",
    "# Generate heatmaps\n",
    "for symbol in symbols:\n",
    "  create_correlation_heatmap(symbol, price_intervals, sentiment_intervals, price_data_path, sentiment_data_path)\n",
    "# Generate top correlation tables\n",
    "for symbol in symbols:\n",
    "    create_top_correlations_table(symbol, price_data_path, sentiment_data_path)\n",
    "# Generate the intraday sentiment chart\n",
    "create_intraday_sentiment_chart(sentiment_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ta in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ta) (2.1.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ta) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aliciali/Library/Python/3.12/lib/python/site-packages (from pandas->ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->ta) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->ta) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aliciali/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the 'ta' package\n",
    "%pip install ta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing BTCUSDT\n",
      "\n",
      "Results for BTCUSDT:\n",
      "Initial Balance: $10,000.00\n",
      "Final Balance: $10562.24\n",
      "Total Return: 5.62%\n",
      "Sharpe Ratio: 1.21\n",
      "Max Drawdown: -1.67%\n",
      "Win Rate: 60.87%\n",
      "Number of trades: 47\n",
      "\n",
      "First 3 trades:\n",
      "{'date': Timestamp('2024-03-20 00:00:00'), 'type': 'open', 'price': np.float64(67840.51), 'quantity': np.float64(0.029480910447164978), 'amount': 2000.0, 'balance': 10000.0}\n",
      "{'date': Timestamp('2024-03-21 00:00:00'), 'type': 'close', 'price': np.float64(65501.27), 'pnl': np.float64(-68.96292495442614), 'balance': np.float64(9931.037075045573), 'reason': 'sl_tp'}\n",
      "{'date': Timestamp('2024-04-15 00:00:00'), 'type': 'open', 'price': np.float64(63419.99), 'quantity': np.float64(0.03131831800996996), 'amount': np.float64(1986.2074150091148), 'balance': np.float64(9931.037075045573)}\n",
      "\n",
      "==================================================\n",
      "Processing ETHUSDT\n",
      "\n",
      "Results for ETHUSDT:\n",
      "Initial Balance: $10,000.00\n",
      "Final Balance: $10902.26\n",
      "Total Return: 9.02%\n",
      "Sharpe Ratio: 1.44\n",
      "Max Drawdown: -1.99%\n",
      "Win Rate: 53.85%\n",
      "Number of trades: 104\n",
      "\n",
      "First 3 trades:\n",
      "{'date': Timestamp('2024-01-23 00:00:00'), 'type': 'open', 'price': np.float64(2242.6), 'quantity': np.float64(0.6688664942477481), 'amount': 1500, 'balance': 10000.0}\n",
      "{'date': Timestamp('2024-01-29 00:00:00'), 'type': 'close', 'price': np.float64(2317.6), 'pnl': np.float64(-50.16498706858111), 'balance': np.float64(9949.835012931419), 'reason': 'sl_tp'}\n",
      "{'date': Timestamp('2024-02-09 00:00:00'), 'type': 'open', 'price': np.float64(2486.56), 'quantity': np.float64(0.603243034553761), 'amount': 1500, 'balance': np.float64(9949.835012931419)}\n",
      "\n",
      "==================================================\n",
      "Processing DOGEUSDT\n",
      "\n",
      "Results for DOGEUSDT:\n",
      "Initial Balance: $10,000.00\n",
      "Final Balance: $9898.28\n",
      "Total Return: -1.02%\n",
      "Sharpe Ratio: -0.44\n",
      "Max Drawdown: -1.77%\n",
      "Win Rate: 54.55%\n",
      "Number of trades: 22\n",
      "\n",
      "First 3 trades:\n",
      "{'date': Timestamp('2024-02-14 00:00:00'), 'type': 'open', 'price': np.float64(0.0855), 'quantity': np.float64(5847.953216374269), 'amount': 500, 'balance': 10000.0}\n",
      "{'date': Timestamp('2024-02-17 00:00:00'), 'type': 'close', 'price': np.float64(0.08356), 'pnl': np.float64(11.345029239766147), 'balance': np.float64(10011.345029239767), 'reason': 'sl_tp'}\n",
      "{'date': Timestamp('2024-02-19 00:00:00'), 'type': 'open', 'price': np.float64(0.08942), 'quantity': np.float64(5591.590248266607), 'amount': 500, 'balance': np.float64(10011.345029239767)}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "class BaseStrategy:\n",
    "    def __init__(self):\n",
    "        self.lookback = 20\n",
    "        self.volume_threshold = 1.2\n",
    "        self.position_size = 0.2  # Base position size 20%\n",
    "        \n",
    "    def calculate_base_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate common indicators for all strategies\"\"\"\n",
    "        # Price indicators\n",
    "        df['price_ma'] = df['close'].rolling(self.lookback).mean()\n",
    "        df['price_std'] = df['close'].rolling(self.lookback).std()\n",
    "        df['price_momentum'] = df['close'].pct_change(3)\n",
    "        \n",
    "        # Volume indicators\n",
    "        df['volume_ma'] = df['volume'].rolling(self.lookback).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "        \n",
    "        # Trend strength\n",
    "        df['trend_strength'] = abs(df['price_momentum'].rolling(5).mean())\n",
    "        \n",
    "        return df\n",
    "\n",
    "class BTCStrategy(BaseStrategy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_loss = 0.02      # 2% stop loss\n",
    "        self.take_profit = 0.03    # 3% take profit\n",
    "        self.max_position = 2000   # Maximum $2000 per trade\n",
    "        \n",
    "    def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.calculate_base_indicators(df)\n",
    "        df['signal'] = 0\n",
    "        \n",
    "        # Calculate specific indicators\n",
    "        df['rsi'] = ta.momentum.RSIIndicator(df['close']).rsi()\n",
    "        df['macd'] = ta.trend.MACD(df['close']).macd()\n",
    "        \n",
    "        # Long conditions (Multiple confirmations)\n",
    "        long_conditions = (\n",
    "            (df['close'] > df['price_ma']) &          # Price above MA\n",
    "            (df['rsi'] < 65) &                        # Not overbought\n",
    "            (df['macd'] > 0) &                        # MACD positive\n",
    "            (df['volume_ratio'] > self.volume_threshold)  # Good volume\n",
    "        )\n",
    "        \n",
    "        # Short conditions\n",
    "        short_conditions = (\n",
    "            (df['close'] < df['price_ma']) &          # Price below MA\n",
    "            (df['rsi'] > 35) &                        # Not oversold\n",
    "            (df['macd'] < 0) &                        # MACD negative\n",
    "            (df['volume_ratio'] > self.volume_threshold)  # Good volume\n",
    "        )\n",
    "        \n",
    "        df.loc[long_conditions, 'signal'] = 1\n",
    "        df.loc[short_conditions, 'signal'] = -1\n",
    "        \n",
    "        return df\n",
    "\n",
    "class ETHStrategy(BaseStrategy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_loss = 0.015     # 1.5% stop loss\n",
    "        self.take_profit = 0.025   # 2.5% take profit\n",
    "        self.max_position = 1500   # Maximum $1500 per trade\n",
    "        \n",
    "    def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.calculate_base_indicators(df)\n",
    "        df['signal'] = 0\n",
    "        \n",
    "        # Calculate specific indicators\n",
    "        df['ema_fast'] = ta.trend.EMAIndicator(df['close'], window=10).ema_indicator()\n",
    "        df['ema_slow'] = ta.trend.EMAIndicator(df['close'], window=21).ema_indicator()\n",
    "        df['atr'] = ta.volatility.AverageTrueRange(df['high'], df['low'], df['close']).average_true_range()\n",
    "        \n",
    "        # Long conditions (more lenient)\n",
    "        long_conditions = (\n",
    "            (df['ema_fast'] > df['ema_slow']) &\n",
    "            (df['close'] > df['price_ma']) &\n",
    "            (df['volume_ratio'] > self.volume_threshold)\n",
    "        )\n",
    "        \n",
    "        # Short conditions (more lenient)\n",
    "        short_conditions = (\n",
    "            (df['ema_fast'] < df['ema_slow']) &\n",
    "            (df['close'] < df['price_ma']) &\n",
    "            (df['volume_ratio'] > self.volume_threshold)\n",
    "        )\n",
    "        \n",
    "        df.loc[long_conditions, 'signal'] = 1\n",
    "        df.loc[short_conditions, 'signal'] = -1\n",
    "        \n",
    "        return df\n",
    "\n",
    "class DOGEStrategy(BaseStrategy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_loss = 0.005       # 0.5% stop loss\n",
    "        self.take_profit = 0.015     # 1.5% take profit\n",
    "        self.max_position = 500      # Smaller position size\n",
    "        self.lookback = 20           # Longer lookback to identify sentiment patterns\n",
    "        \n",
    "    def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.calculate_base_indicators(df)\n",
    "        df['signal'] = 0\n",
    "        \n",
    "        # Volume patterns\n",
    "        df['volume_ma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "        \n",
    "        # Price patterns\n",
    "        df['high_low_range'] = (df['high'] - df['low']) / df['low']\n",
    "        df['price_velocity'] = df['close'].pct_change(3)\n",
    "        df['price_acceleration'] = df['price_velocity'].diff()\n",
    "        \n",
    "        # Volatility\n",
    "        df['volatility'] = df['close'].pct_change().rolling(5).std()\n",
    "        df['volatility_ma'] = df['volatility'].rolling(20).mean()\n",
    "        \n",
    "        # Identify potential bottoms (based on Very Low sentiment success)\n",
    "        df['lower_band'] = df['close'].rolling(10).min()\n",
    "        df['upper_band'] = df['close'].rolling(10).max()\n",
    "        df['price_position'] = (df['close'] - df['lower_band']) / (df['upper_band'] - df['lower_band'])\n",
    "        \n",
    "        # Buy signals - looking for oversold conditions\n",
    "        long_conditions = (\n",
    "            (df['price_position'] < 0.2) &                   # Price near recent lows\n",
    "            (df['volume_ratio'] > 1.5) &                     # Above average volume\n",
    "            (df['volatility'] < df['volatility_ma']) &       # Lower volatility\n",
    "            (df['price_acceleration'] > 0)                   # Momentum turning positive\n",
    "        )\n",
    "        \n",
    "        # Sell signals - take profits on strength\n",
    "        short_conditions = (\n",
    "            (df['price_position'] > 0.8) &                  # Price near recent highs\n",
    "            (df['volume_ratio'] > 1.5) &                    # Above average volume\n",
    "            (df['volatility'] > df['volatility_ma'] * 1.2)  # Higher volatility\n",
    "        )\n",
    "        \n",
    "        # Apply signals\n",
    "        df.loc[long_conditions, 'signal'] = 1\n",
    "        df.loc[short_conditions, 'signal'] = -1\n",
    "        \n",
    "        # Dynamic position sizing based on confidence\n",
    "        df['position_size'] = 0.0\n",
    "        \n",
    "        # Base position size inversely proportional to price position\n",
    "        df.loc[long_conditions, 'position_size'] = 0.05 + (0.05 * (1 - df['price_position']))\n",
    "        df.loc[short_conditions, 'position_size'] = 0.05 + (0.05 * df['price_position'])\n",
    "        \n",
    "        # Adjust for volume confirmation\n",
    "        df.loc[df['signal'] != 0, 'position_size'] *= df['volume_ratio'].clip(0.5, 2.0)\n",
    "        \n",
    "        # Final position size limits\n",
    "        df['position_size'] = df['position_size'].clip(0.05, 0.15)\n",
    "        \n",
    "        # Exit conditions\n",
    "        exit_conditions = (\n",
    "            (df['volatility'] > df['volatility_ma'] * 2) |  # Extreme volatility\n",
    "            (df['volume_ratio'] < 0.5)                      # Volume dying\n",
    "        )\n",
    "        df.loc[exit_conditions & (df['signal'] != 0), 'signal'] = 0\n",
    "        \n",
    "        return df\n",
    "\n",
    "def backtest_strategy(df: pd.DataFrame, strategy: BaseStrategy, initial_capital: float = 10000.0) -> Dict:\n",
    "    \"\"\"Backtest a strategy with proper position sizing and risk management\"\"\"\n",
    "    balance = initial_capital\n",
    "    position = 0\n",
    "    entry_price = 0\n",
    "    entry_quantity = 0\n",
    "    trades = []\n",
    "    equity_curve = [initial_capital]\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        current_price = df['close'].iloc[i]\n",
    "        signal = df['signal'].iloc[i]\n",
    "        date = pd.to_datetime(df['timestamp'].iloc[i])\n",
    "        \n",
    "        # Check stop loss and take profit if in position\n",
    "        if position != 0:\n",
    "            pnl_pct = (current_price - entry_price) / entry_price * position\n",
    "            if pnl_pct <= -strategy.stop_loss or pnl_pct >= strategy.take_profit:\n",
    "                # Close position with proper risk management\n",
    "                pnl = (current_price - entry_price) * entry_quantity * position\n",
    "                balance += pnl\n",
    "                position = 0\n",
    "                trades.append({\n",
    "                    'date': date,\n",
    "                    'type': 'close',\n",
    "                    'price': current_price,\n",
    "                    'pnl': pnl,\n",
    "                    'balance': balance,\n",
    "                    'reason': 'sl_tp'\n",
    "                })\n",
    "        \n",
    "        # Process signals\n",
    "        if position == 0 and signal != 0:\n",
    "            # Calculate position size based on risk\n",
    "            risk_amount = balance * strategy.stop_loss\n",
    "            max_trade_amount = min(balance * strategy.position_size, strategy.max_position)\n",
    "            position_amount = min(max_trade_amount, risk_amount / strategy.stop_loss)\n",
    "            \n",
    "            # Calculate quantity\n",
    "            quantity = position_amount / current_price\n",
    "            \n",
    "            position = signal\n",
    "            entry_price = current_price\n",
    "            entry_quantity = quantity\n",
    "            \n",
    "            trades.append({\n",
    "                'date': date,\n",
    "                'type': 'open',\n",
    "                'price': current_price,\n",
    "                'quantity': quantity,\n",
    "                'amount': position_amount,\n",
    "                'balance': balance\n",
    "            })\n",
    "        \n",
    "        # Update equity curve\n",
    "        current_equity = balance\n",
    "        if position != 0:\n",
    "            unrealized_pnl = (current_price - entry_price) * entry_quantity * position\n",
    "            current_equity += unrealized_pnl\n",
    "        equity_curve.append(current_equity)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    returns = pd.Series(equity_curve).pct_change().dropna()\n",
    "    \n",
    "    # Calculate max drawdown safely\n",
    "    def calculate_max_drawdown(equity_series):\n",
    "        if not equity_series:\n",
    "            return 0\n",
    "        rolling_max = pd.Series(equity_series).expanding().max()\n",
    "        drawdowns = pd.Series(equity_series) / rolling_max - 1\n",
    "        return drawdowns.min() * 100\n",
    "    \n",
    "    return {\n",
    "        'final_balance': equity_curve[-1],\n",
    "        'total_return': (equity_curve[-1] / initial_capital - 1) * 100,\n",
    "        'sharpe_ratio': np.sqrt(252) * returns.mean() / returns.std() if len(returns) > 0 and returns.std() != 0 else 0, # Annualized Sharpe ratio for daily data\n",
    "        'max_drawdown': calculate_max_drawdown(equity_curve),\n",
    "        'win_rate': len([t for t in trades if t.get('pnl', 0) > 0]) / len([t for t in trades if 'pnl' in t]) * 100 if trades else 0,\n",
    "        'trades': trades,\n",
    "        'equity_curve': equity_curve\n",
    "    }\n",
    "\n",
    "def run_strategies(price_data_path: str):\n",
    "    \"\"\"Run all strategies with detailed reporting\"\"\"\n",
    "    strategies = {\n",
    "        'BTCUSDT': BTCStrategy(),\n",
    "        'ETHUSDT': ETHStrategy(),\n",
    "        'DOGEUSDT': DOGEStrategy()\n",
    "    }\n",
    "    \n",
    "    for symbol, strategy in strategies.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {symbol}\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(f\"{price_data_path}{symbol}_1d.csv\")\n",
    "        df = strategy.generate_signals(df)\n",
    "        \n",
    "        # Run backtest\n",
    "        results = backtest_strategy(df, strategy)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults for {symbol}:\")\n",
    "        print(f\"Initial Balance: $10,000.00\")\n",
    "        print(f\"Final Balance: ${results['final_balance']:.2f}\")\n",
    "        print(f\"Total Return: {results['total_return']:.2f}%\")\n",
    "        print(f\"Sharpe Ratio: {results['sharpe_ratio']:.2f}\")\n",
    "        print(f\"Max Drawdown: {results['max_drawdown']:.2f}%\")\n",
    "        print(f\"Win Rate: {results['win_rate']:.2f}%\")\n",
    "        print(f\"Number of trades: {len(results['trades'])}\")\n",
    "        \n",
    "        if results['trades']:\n",
    "            print(\"\\nFirst 3 trades:\")\n",
    "            for trade in results['trades'][:3]:\n",
    "                print(trade)\n",
    "        \n",
    "# Run the analysis\n",
    "price_data_path = \"price_data/\"\n",
    "run_strategies(price_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
